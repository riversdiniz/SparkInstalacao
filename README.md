# üöÄ Utilizando o Spark no Windows
[fonte](https://spark.apache.org/docs/3.1.2/api/python/getting_started/install.html)

### üìã Pr√©-requisitos
#### Passo 1 - Instalando o Java
O PySpark requer a instala√ß√£o do Java na vers√£o 7 ou superior. Obtenha a vers√£o mais recente clicando [aqui](https://www.java.com/pt-BR/download/). Para verificar a vers√£o que est√° instalada em sua m√°quina execute a seguinte linha de c√≥digo no seu prompt:
```
java -version
```
### Passo 2 - Instalando o Python
O Python deve ser instalado em sua vers√£o 2.6 ou superior. Para obter a vers√£o mais recente clique [aqui](https://www.python.org/downloads/windows/). Para verificar a vers√£o do Python que est√° instalada em sua m√°quina digite o seguinte comando em seu prompt:
```
python --version
```
#### Passo 3 - Instalando o Apache Spark
Selecione a vers√£o mais est√°vel clicando [aqui](http://spark.apache.org/downloads.html). Na cria√ß√£o deste projeto utilizamos a vers√£o do Spark 3.1.2 e como tipo de pacote selecionamos Pre-built for Apache Hadoop 2.7.

Para instalar o Apache Spark n√£o √© necess√°rio executar um instalador, basta descomprimir os arquivos em uma pasta de sua escolha.

`
Obs.: certifique-se de que o caminho onde os arquivos do Spark foram armazenados n√£o contenham espa√ßos (ex.: "C:\spark\spark-3.1.2-bin-hadoop2.7").
`

Para testar o funcionamento do Spark execute os comandos abaixo em seu prompt de comando. Esses comandos assumem que voc√™ extraiu os arquivos do Spark na pasta "C:\spark".

cd C:\spark\spark-3.1.2-bin-hadoop2.7
bin\pyspark
O comando acima inicia o shell do PySpark que permite trabalhar interativamente com o Spark.

Para sair basta digitar exit() e logo depois presionar Enter. Para voltar ao prompt pressione Enter novamente.

#### Passo 4 - Instalando o findspark
```
pip install findspark
```
#### Passo 5 - Instalando o winutils
Os arquivos do Spark n√£o incluem o utilit√°rio winutils.exe que √© utilizado pelo Spark no Windows. Se n√£o informar onde o Spark deve procurar este utilit√°rio, veremos alguns erros no console e tamb√©m n√£o conseguiremos executar scripts Python utilizando o utilit√°rio spark-submit.

Fa√ßa o [download](https://github.com/steveloughran/winutils) para a vers√£o do Hadoop para a qual sua instala√ß√£o do Spark foi constru√≠da. Em nosso exemplo foi utilizada a [vers√£o 2.7](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin). Fa√ßa o download apenas do arquivo winutils.exe.

Crie a pasta "hadoop\bin" dentro da pasta que cont√©m os arquivos do Spark (em nosso exemplo "C:\spark\spark-3.1.2-bin-hadoop2.7") e copie o arquivo winutils.exe para dentro desta pasta.

Crie duas vari√°veis de ambiente no seu Windows. A primeira chamada SPARK_HOME que aponta para a pasta onde os arquivos Spark foram armazenados (em nosso exemplo "C:\spark\spark-3.1.2-bin-hadoop2.7"). A segunda chamada HADOOP_HOME que aponta para %SPARK_HOME%\hadoop (assim podemos modificar SPARK_HOME sem precisar alterar HADOOP_HOME).

## üõ†Ô∏è Projeto
Nosso projeto consiste em ler, manipular, tratar e salvar um conjunto de dados volumosos utilizando como ferramenta o Spark.

Carregamento de dados

Dados P√∫blicos CNPJ

Receita Federal
* [Empresas](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/empresas.zip)
* [Estabelecimentos](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/estabelecimentos.zip)
* [S√≥cios](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/socios.zip)

[Fonte original dos dados](https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/dados-publicos-cnpj)

‚å®Ô∏è üöÄ por [River Diniz](https://gist.github.com/riversdiniz) üßë‚ÄçüöÄ
